{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "144c6fc5",
   "metadata": {
    "tags": [
     "header"
    ]
   },
   "source": [
    "# STA 220 Assignment 2\n",
    "\n",
    "Due __Februrary 9, 2024__ by __11:59pm__. Submit your work by uploading it to Gradescope through Canvas.\n",
    "\n",
    "Instructions:\n",
    "\n",
    "1. Provide your solutions in new cells following each exercise description. Create as many new cells as necessary. Use code cells for your Python scripts and Markdown cells for explanatory text or answers to non-coding questions. Answer all textual questions in complete sentences.\n",
    "2. The use of assistive tools is permitted, but must be indicated. You will be graded on your proficiency in coding. Produce high quality code by adhering to proper programming principles. \n",
    "3. Export the .jpynb as .pdf and submit it on Gradescope in time. To facilitate grading, indicate the area of the solution on the submission. Submissions without indication will be marked down. No late submissions accepted. \n",
    "4. If test cases are given, your solution must be in the same format. \n",
    "5. The total number of points is 10."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bf1eaec1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed \n",
    "import numpy as np\n",
    "from scipy.sparse import csr_matrix\n",
    "import requests_cache\n",
    "from lxml import html as lx\n",
    "import json\n",
    "from urllib.parse import unquote\n",
    "from scipy.sparse import save_npz, load_npz"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe607f59",
   "metadata": {
    "tags": [
     "exercise"
    ]
   },
   "source": [
    "__Exercise 1__\n",
    "\n",
    "We will compute the [PageRank](https://en.wikipedia.org/wiki/PageRank) of the articles of the [Sinhala](https://en.wikipedia.org/wiki/Sinhala_language) wikipedia, which is available at [si.wikipedia.org](https://si.wikipedia.org/wiki/%E0%B6%B8%E0%B7%94%E0%B6%BD%E0%B7%8A_%E0%B6%B4%E0%B7%92%E0%B6%A7%E0%B7%94%E0%B7%80). Additional information of the Sinhala wiki can be found [here](https://meta.wikimedia.org/wiki/List_of_Wikipedias). \n",
    "\n",
    "_Hints: If you don't speak Sinhalese, you might want to learn the wiki logic from the english wikipedia, and translate your findings. Also, caching is highly recommended._"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f276cd17",
   "metadata": {
    "tags": [
     "exercise"
    ]
   },
   "source": [
    "__(a)__ Use the special [AllPages](https://si.wikipedia.org/wiki/%E0%B7%80%E0%B7%92%E0%B7%81%E0%B7%9A%E0%B7%82:%E0%B7%83%E0%B7%92%E0%B6%BA%E0%B7%85%E0%B7%94_%E0%B6%B4%E0%B7%92%E0%B6%A7%E0%B7%94) page and understand its logic to retrieve the url of all articles in the sinhalese wikipedia. Make sure to skip redirections.\n",
    "\n",
    "_How many articles are there?_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "01e6a787",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of articles: 24233\n"
     ]
    }
   ],
   "source": [
    "# Starting URL of the AllPages page on Sinhala Wikipedia\n",
    "url = \"https://si.wikipedia.org/wiki/%E0%B7%80%E0%B7%92%E0%B7%81%E0%B7%9A%E0%B7%82:%E0%B7%83%E0%B7%92%E0%B6%BA%E0%B7%85%E0%B7%94_%E0%B6%B4%E0%B7%92%E0%B6%A7%E0%B7%94\"\n",
    "\n",
    "# Map to store article URLs\n",
    "article_urls = {}\n",
    "\n",
    "# Function to scrape article URLs from a page\n",
    "def scrape_article_urls(url):\n",
    "    headers = {'user-agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/106.0.0.0 Safari/537.36'}\n",
    "    response = requests.get(url, headers=headers)\n",
    "    soup = BeautifulSoup(response.text, \"lxml\")\n",
    "    article_list = soup.find(\"div\", {\"class\": \"mw-allpages-body\"})\n",
    "    articles = article_list.find_all(\"a\", href=True)\n",
    "    for article in articles:\n",
    "        if(article.get('class', [''])[0] == 'mw-redirect'):\n",
    "            continue\n",
    "        article_urls[\"https://si.wikipedia.org\" + article[\"href\"]] = True\n",
    "\n",
    "    return soup\n",
    "\n",
    "# Initial scraping of the first page\n",
    "soup = scrape_article_urls(url)\n",
    "next_page = (soup.find(\"div\", {\"class\": \"mw-allpages-nav\"})).a\n",
    "\n",
    "while True:\n",
    "    next_page_url = \"https://si.wikipedia.org\" + next_page[\"href\"]\n",
    "    soup = scrape_article_urls(next_page_url)\n",
    "    next_pages = soup.find(\"div\", {\"class\": \"mw-allpages-nav\"}).find_all('a')\n",
    "    if(len(next_pages) < 2):\n",
    "        break\n",
    "    else:\n",
    "        next_page = next_pages[1]\n",
    "\n",
    "# Print the total number of articles and a sample of article URLs\n",
    "print(\"Total number of articles:\", len(article_urls))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c90213dd",
   "metadata": {
    "tags": [
     "exercise"
    ]
   },
   "source": [
    "__(b, i)__ Scan all articles in the sinhalese wikipedia and retrieve all links to other articles. Avoid links to special pages, images or the ones that point to another website. Only count the proper article for links that point to a specific section. Use regular expressions to manage these cases. \n",
    "__(ii)__ Make sure to match redirections to their correct destiation article. To this end, find how wikipedia treats redirections and retrieve the true article. _(Help: Try searching for 'uc davis' on en.wikipedia.org')_\n",
    "__(iii)__ Use threading to request all articles and obtain all links to other articles. _(Attention: This takes about thirty minutes!)_\n",
    "\n",
    "\n",
    "_How many links to other articles are there?_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "8c3782df",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_article_links(url):\n",
    "\n",
    "    headers = {'user-agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/106.0.0.0 Safari/537.36'}\n",
    "\n",
    "    try:\n",
    "        with requests_cache.CachedSession('wiki_cache') as session:\n",
    "            response = session.get(url, headers=headers)\n",
    "            response.raise_for_status()\n",
    "            soup = BeautifulSoup(response.text, \"lxml\")\n",
    "\n",
    "            article_content = soup.find(\"div\", {\"class\": \"mw-parser-output\"})\n",
    "            links = article_content.find_all('a')\n",
    "\n",
    "            all_links = []\n",
    "\n",
    "            ## Excluding special pages and including pages that start with wither /wiki/ or https://si.wikipedia.org/\n",
    "            pattern = re.compile(r'^https?://si\\.wikipedia\\.org/wiki/[^:]+$|^/wiki/[^:]+$')\n",
    "\n",
    "            for link in links:\n",
    "                href = link.get('href', '')\n",
    "                if(pattern.match(href)):\n",
    "\n",
    "                    href = href.split('#')[0]\n",
    "\n",
    "                    if(not href.startswith(\"http\")):\n",
    "                        new_url = f'https://si.wikipedia.org{href}'\n",
    "                    else:\n",
    "                        new_url = href\n",
    "\n",
    "                    if(new_url == url):\n",
    "                        continue\n",
    "\n",
    "                    if(new_url in article_urls):\n",
    "                        all_links.append(new_url)\n",
    "                    elif(link.get('class', [''])[0] == 'mw-redirect'): ## mapping redirections to their final destinations\n",
    "                        redirect_url = session.get(new_url, headers=headers, allow_redirects=True)\n",
    "                        redirect_content = lx.fromstring(redirect_url.content)\n",
    "                        actual_link = redirect_content.xpath('//span[@class=\"mw-redirectedfrom\"]/a/@href')\n",
    "                        if actual_link and actual_link[0] in article_urls:\n",
    "                                all_links.append(actual_link[0]) \n",
    "            \n",
    "            return all_links\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error scraping URL {url}: {e}\")\n",
    "        return []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "7e877c19",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_articles_links_with_threading(article_urls):\n",
    "    links_dict = {}\n",
    "    with ThreadPoolExecutor(max_workers=8) as executor:\n",
    "        future_to_url = {executor.submit(scrape_article_links, url): url for url in article_urls}\n",
    "        for future in as_completed(future_to_url):\n",
    "            url = future_to_url[future]\n",
    "            try:\n",
    "                links = future.result()\n",
    "                links_dict[url] = links\n",
    "            except Exception as e:\n",
    "                print(f\"Error processing URL {url}: {e}\")\n",
    "\n",
    "    return links_dict\n",
    "\n",
    "requests_cache.install_cache('wiki_cache') \n",
    "links_dict = get_articles_links_with_threading(article_urls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "59200f01",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of links: 333952\n"
     ]
    }
   ],
   "source": [
    "count = 0\n",
    "\n",
    "for links in links_dict.values():\n",
    "    count += len(links)\n",
    "\n",
    "print(\"Total number of links:\", count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "55e744dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('links_dict.json', 'w') as json_file:\n",
    "    json.dump(links_dict, json_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "a129ab0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('links_dict.json', 'r') as json_file:\n",
    "    links_dict = json.load(json_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "8025bc1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removing duplicates in destination to speed up computation\n",
    "for url, links in links_dict.items():\n",
    "    links_dict[url] = set(links)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1775e365",
   "metadata": {
    "tags": [
     "exercise"
    ]
   },
   "source": [
    "__(c)__ Compute the transition matrix (see [here](https://en.wikipedia.org/wiki/Google_matrix) and [here](https://www.amsi.org.au/teacher_modules/pdfs/Maths_delivers/Pagerank5.pdf) for step-by-step instructions). Make sure to tread dangling nodes. You may want to use: \n",
    "```\n",
    "from scipy.sparse import csr_matrix\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "cbf96d01",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_transition_matrix(links_dict, link_to_idx):\n",
    "    \n",
    "    total_articles = len(link_to_idx)  # Number of nodes\n",
    "    rows, cols, data = [], [], []\n",
    "\n",
    "    for source, destinations in links_dict.items():\n",
    "        \n",
    "        if len(destinations) == 0: ## if it is a dangling node\n",
    "            destinations = set(links_dict.keys())\n",
    "\n",
    "        source_index = link_to_idx[source]\n",
    "        link_probability = 1.0 / len(destinations)\n",
    "\n",
    "        for destination in destinations:\n",
    "            destination_index = link_to_idx[destination]\n",
    "            data.append(link_probability)\n",
    "            cols.append(source_index)\n",
    "            rows.append(destination_index)\n",
    "\n",
    "    matrix = csr_matrix((data, (rows, cols)), shape=(total_articles, total_articles))\n",
    "\n",
    "    return matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "e04af9f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "link_to_idx = {link: index for index, link in enumerate(links_dict.keys())}\n",
    "transition_matrix = create_transition_matrix(links_dict, link_to_idx)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be68fe99",
   "metadata": {
    "tags": [
     "exercise"
    ]
   },
   "source": [
    "__(d, i)__ Set the damping factor to `0.85` and compute the PageRank for each article, using fourty iterations and starting with a vector with equal entries. __(ii)__ Obtain the top ten articles in terms of PageRank, and, retrieving the articles again, find the correponding English article, if available. \n",
    "\n",
    "_Return the corresponding English article titles of the top ten articles from the Sinhalese wikipedia._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "9a26a7ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_damping_factor(matrix, damping_factor=0.85):\n",
    "\n",
    "    n = matrix.shape[0]\n",
    "\n",
    "    data = [(1 - damping_factor) / n] * n\n",
    "    rows = [i for i in range(n)]\n",
    "    cols = [i for i in range(n)]\n",
    "\n",
    "    damping_matrix = csr_matrix((data, (rows, cols)), shape=(n, n))\n",
    "\n",
    "    matrix = damping_factor * matrix + damping_matrix\n",
    "\n",
    "    return matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "a5657889",
   "metadata": {},
   "outputs": [],
   "source": [
    "transition_matrix_with_damping_factor = apply_damping_factor(transition_matrix)\n",
    "save_npz('transition_matrix_with_damping_factor.npz', transition_matrix_with_damping_factor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "673de146",
   "metadata": {},
   "outputs": [],
   "source": [
    "transition_matrix_with_damping_factor = load_npz('transition_matrix_with_damping_factor.npz')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "728d0adb",
   "metadata": {},
   "outputs": [],
   "source": [
    "n = transition_matrix_with_damping_factor.shape[0]  # Number of articles\n",
    "\n",
    "# PageRank vector\n",
    "pagerank_vector = np.ones(n) / n  # Equal entries\n",
    "\n",
    "for _ in range(40):\n",
    "    pagerank_vector = transition_matrix_with_damping_factor.dot(pagerank_vector)\n",
    "\n",
    "top_ten_indices = np.argsort(pagerank_vector)[-10:][::-1]  # Indices of top ten articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "a5d0c057",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_english_article(si_article_url):\n",
    "    \n",
    "    article_title = si_article_url.split('/')[-1]\n",
    "    article_title = unquote(article_title)\n",
    "    \n",
    "    # Wikipedia API endpoint for language links\n",
    "    api_url = \"https://si.wikipedia.org/w/api.php\"\n",
    "    \n",
    "    params = {\n",
    "        'action': 'query',\n",
    "        'prop': 'langlinks',\n",
    "        'titles': article_title,\n",
    "        'lllang': 'en',\n",
    "        'format': 'json'\n",
    "    }\n",
    "    \n",
    "    response = requests.get(api_url, params=params)\n",
    "    data = response.json()\n",
    "    \n",
    "    page_id = next(iter(data['query']['pages']))\n",
    "    \n",
    "    if 'langlinks' in data['query']['pages'][page_id]:\n",
    "        en_title = data['query']['pages'][page_id]['langlinks'][0]['*']\n",
    "        en_title = en_title.replace(' ', '_')\n",
    "        return f\"https://en.wikipedia.org/wiki/{en_title}\"\n",
    "    else:\n",
    "        return \"No corresponding English article found.\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "bf21bd9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "idx_to_link = {idx: link for link, idx in link_to_idx.items()}\n",
    "top_si_articles = []\n",
    "corresponding_en_articles = []\n",
    "for idx in top_ten_indices:\n",
    "    top_si_articles.append(idx_to_link[idx])\n",
    "\n",
    "for si_url in top_si_articles:\n",
    "    corresponding_en_articles.append(get_english_article(si_url))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56f51594",
   "metadata": {},
   "source": [
    "|English|\n",
    "|:-|\n",
    "|https://en.wikipedia.org/wiki/Sri_Lanka|\n",
    "|https://en.wikipedia.org/wiki/English_language|\n",
    "|https://en.wikipedia.org/wiki/Gross_domestic_product|\n",
    "|https://en.wikipedia.org/wiki/United_Kingdom|\n",
    "|https://en.wikipedia.org/wiki/Geographic_coordinate_system|\n",
    "|https://en.wikipedia.org/wiki/List_of_decades,_centuries,_and_millennia|\n",
    "|https://en.wikipedia.org/wiki/Economics|\n",
    "|https://en.wikipedia.org/wiki/Purchasing_power_parity|\n",
    "|https://tinyurl.com/englishLanguageAndRace (No corresponding English article found)|\n",
    "|https://tinyurl.com/centuryList (No corresponding English article found)|"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e8a4f78",
   "metadata": {},
   "source": [
    "|Sinhala|English|\n",
    "|:-|:-|\n",
    "|https://si.wikipedia.org/wiki/ශ්%E2%80%8Dරී_ලංකාව|https://en.wikipedia.org/wiki/Sri_Lanka|\n",
    "|https://si.wikipedia.org/wiki/ඉංග්%E2%80%8Dරීසි_භාෂාව|https://en.wikipedia.org/wiki/English_language|\n",
    "|https://si.wikipedia.org/wiki/දළ_දේශීය_නිෂ්පාදිතය|https://en.wikipedia.org/wiki/Gross_domestic_product|\n",
    "|https://si.wikipedia.org/wiki/එක්සත්_රාජධානිය| https://en.wikipedia.org/wiki/United_Kingdom|\n",
    "|https://si.wikipedia.org/wiki/භූගෝලීය_ඛණ්ඩාංක_පද්ධතිය| https://en.wikipedia.org/wiki/Geographic_coordinate_system|\n",
    "|https://si.wikipedia.org/wiki/දශක_ලැයිස්තුව|https://en.wikipedia.org/wiki/List_of_decades,_centuries,_and_millennia|\n",
    "|https://si.wikipedia.org/wiki/ආර්ථික_විද්%E2%80%8Dයාව | https://en.wikipedia.org/wiki/Economics|\n",
    "|https://si.wikipedia.org/wiki/ක්%E2%80%8Dරය_ශක්ති_සාම්%E2%80%8Dයය| https://en.wikipedia.org/wiki/Purchasing_power_parity|\n",
    "|https://si.wikipedia.org/wiki/ඉංග්%E2%80%8Dරීසි| No corresponding English article found.|\n",
    "|https://si.wikipedia.org/wiki/සියවස්_ලැයිස්තුව|No corresponding English article found.|"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2078477d",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Tags",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
